# P2 Gateway API Design

**Date:** 2026-01-27
**Status:** Design approved
**Implements:** P2_IMPLEMENTATION_SPEC.md

---

## Overview

P2 exposes Chinvex as a remote, TLS-reachable API for ChatGPT Actions. The gateway is read-only, token-authenticated, and security-hardened from day one.

### Architecture

```
┌─────────────┐
│  ChatGPT    │
│  Actions    │
└──────┬──────┘
       │ HTTPS (Bearer token)
       ▼
┌─────────────────────┐
│ Cloudflare Tunnel   │  ← or Caddy
│ TLS termination     │
└──────┬──────────────┘
       │ HTTP localhost:7778
       ▼
┌─────────────────────┐
│ FastAPI Gateway     │
│ ┌─────────────────┐ │
│ │ Auth Middleware │ │ ← Token verification
│ ├─────────────────┤ │
│ │ Rate Limiter    │ │ ← In-memory token bucket
│ ├─────────────────┤ │
│ │ Validator       │ │ ← Input validation
│ ├─────────────────┤ │
│ │ Endpoints       │ │ ← /v1/evidence, /v1/search, etc.
│ └─────────────────┘ │
└──────┬──────────────┘
       │
       ▼
┌─────────────────────┐
│ Chinvex Core        │
│ (existing search,   │
│  retrieval logic)   │
└─────────────────────┘
```

**Key principle:** Gateway is a thin security + routing layer over existing Chinvex functionality.

---

## Security Layer

### 1. Authentication

**Bearer token with constant-time comparison:**

```python
# src/chinvex/gateway/auth.py

import secrets
from fastapi import HTTPException, Security
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer()

def verify_token(
    credentials: HTTPAuthorizationCredentials = Security(security)
) -> str:
    """Verify bearer token using constant-time comparison."""
    expected = get_gateway_config().token
    if not expected:
        raise HTTPException(
            status_code=500,
            detail="Gateway token not configured (CHINVEX_API_TOKEN missing)"
        )

    provided = credentials.credentials
    if not secrets.compare_digest(provided, expected):
        raise HTTPException(
            status_code=401,
            detail="Invalid authentication token"
        )

    return provided
```

**Properties:**
- Constant-time comparison prevents timing attacks
- No token in error messages
- All endpoints except `/health` require auth

### 2. Input Validation

**Pydantic models with validators:**

```python
# src/chinvex/gateway/validation.py

import re
from pydantic import BaseModel, field_validator

CONTEXT_PATTERN = re.compile(r'^[A-Za-z0-9_-]{1,50}$')
CHUNK_ID_PATTERN = re.compile(r'^[a-f0-9]{12}$')

class EvidenceRequest(BaseModel):
    context: str
    query: str
    k: int = 8

    @field_validator('context')
    def validate_context(cls, v):
        if not CONTEXT_PATTERN.match(v):
            raise ValueError('Invalid context name format')
        return v

    @field_validator('query')
    def validate_query(cls, v):
        if not v.strip():
            raise ValueError('Query cannot be empty')
        if len(v) > 1000:
            raise ValueError('Query exceeds 1000 character limit')
        if '\x00' in v:
            raise ValueError('Null bytes not allowed in query')
        return v.strip()

    @field_validator('k')
    def validate_k(cls, v):
        if v < 1 or v > 20:
            raise ValueError('k must be between 1 and 20')
        return v
```

**Validation rules:**
- Context name: alphanumeric + underscore/hyphen only (prevents path traversal)
- Query: UTF-8, no null bytes, max 1000 chars
- k: capped at 20 (prevents resource exhaustion)

### 3. Rate Limiting

**In-memory token bucket:**

```python
# src/chinvex/gateway/rate_limit.py

from dataclasses import dataclass
from datetime import datetime, timedelta
from collections import defaultdict
from fastapi import HTTPException

@dataclass
class TokenBucket:
    requests_per_minute: int
    requests_per_hour: int
    minute_tokens: int = 0
    hour_tokens: int = 0
    minute_reset: datetime = None
    hour_reset: datetime = None

class RateLimiter:
    def __init__(self, config: dict):
        self.rpm = config.get('requests_per_minute', 60)
        self.rph = config.get('requests_per_hour', 500)
        self.buckets = defaultdict(lambda: TokenBucket(self.rpm, self.rph))

    def check_limit(self, token: str) -> None:
        """Check rate limits. Raises 429 if exceeded."""
        now = datetime.now()
        bucket = self.buckets[token]

        # Reset minute bucket if needed
        if bucket.minute_reset is None or now >= bucket.minute_reset:
            bucket.minute_tokens = self.rpm
            bucket.minute_reset = now + timedelta(minutes=1)

        # Reset hour bucket if needed
        if bucket.hour_reset is None or now >= bucket.hour_reset:
            bucket.hour_tokens = self.rph
            bucket.hour_reset = now + timedelta(hours=1)

        # Check limits
        if bucket.minute_tokens <= 0:
            retry_after = int((bucket.minute_reset - now).total_seconds())
            raise HTTPException(
                status_code=429,
                detail="Rate limit exceeded (per-minute)",
                headers={"Retry-After": str(retry_after)}
            )

        if bucket.hour_tokens <= 0:
            retry_after = int((bucket.hour_reset - now).total_seconds())
            raise HTTPException(
                status_code=429,
                detail="Rate limit exceeded (per-hour)",
                headers={"Retry-After": str(retry_after)}
            )

        # Consume tokens
        bucket.minute_tokens -= 1
        bucket.hour_tokens -= 1
```

**Strategy:**
- In-memory (P2 scope - Redis is P3)
- Per-token limits
- Separate minute and hour buckets
- Returns `Retry-After` header

### 4. Audit Logging

**Append-only JSONL log:**

```python
# src/chinvex/gateway/audit.py

import json
import hashlib
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict

@dataclass
class AuditEntry:
    ts: str
    request_id: str
    endpoint: str
    context: str | None
    query_hash: str | None  # SHA256, not raw text
    status: int
    latency_ms: int
    client_ip: str | None

class AuditLogger:
    def __init__(self, log_path: str):
        self.log_path = Path(log_path)
        self.log_path.parent.mkdir(parents=True, exist_ok=True)

    def log(
        self,
        request_id: str,
        endpoint: str,
        status: int,
        latency_ms: int,
        context: str | None = None,
        query: str | None = None,
        client_ip: str | None = None
    ):
        """Append audit entry. Never logs raw query text."""
        entry = AuditEntry(
            ts=datetime.utcnow().isoformat() + "Z",
            request_id=request_id,
            endpoint=endpoint,
            context=context,
            query_hash=self._hash_query(query) if query else None,
            status=status,
            latency_ms=latency_ms,
            client_ip=client_ip
        )

        with open(self.log_path, "a") as f:
            f.write(json.dumps(asdict(entry), default=str) + "\n")

    @staticmethod
    def _hash_query(query: str) -> str:
        return "sha256:" + hashlib.sha256(query.encode()).hexdigest()[:16]
```

**Privacy principles:**
- Never log raw query text
- Never log token values
- Never log full response bodies
- DO log: timestamp, endpoint, status, latency, query hash

---

## Endpoints

### GET /health

**No authentication required.**

```json
{
  "status": "ok",
  "version": "0.2.0",
  "contexts_available": 3
}
```

### POST /v1/search

**Raw hybrid search. Returns ranked chunks without grounding.**

Request:
```json
{
  "context": "Chinvex",
  "query": "hybrid retrieval scoring",
  "k": 10,
  "source_types": ["repo", "chat"],
  "no_recency": false
}
```

Response:
```json
{
  "context": "Chinvex",
  "query": "hybrid retrieval scoring",
  "results": [
    {
      "chunk_id": "abc123",
      "text": "def blend_scores(fts_norm, vec_norm)...",
      "source_uri": "C:\\Code\\chinvex\\src\\scoring.py",
      "source_type": "repo",
      "scores": {
        "fts": 0.82,
        "vector": 0.75,
        "blended": 0.79,
        "rank": 0.79
      },
      "metadata": {
        "line_start": 42,
        "line_end": 67,
        "updated_at": "2026-01-26T10:00:00Z"
      }
    }
  ],
  "total_results": 10
}
```

### POST /v1/evidence

**Search with grounding check. Primary endpoint for ChatGPT Actions.**

Request:
```json
{
  "context": "Chinvex",
  "query": "How does score blending work?",
  "k": 8
}
```

Response (grounded=true):
```json
{
  "context": "Chinvex",
  "query": "How does score blending work?",
  "grounded": true,
  "evidence_pack": {
    "chunks": [
      {
        "chunk_id": "abc123",
        "text": "def blend_scores(fts_norm, vec_norm)...",
        "source_uri": "C:\\Code\\chinvex\\src\\scoring.py",
        "source_type": "repo",
        "range": {
          "line_start": 42,
          "line_end": 67
        },
        "score": 0.89
      }
    ]
  },
  "retrieval_debug": {
    "k": 8,
    "chunks_retrieved": 8,
    "chunks_above_threshold": 5
  }
}
```

Response (grounded=false):
```json
{
  "context": "Chinvex",
  "query": "What is the airspeed velocity of an unladen swallow?",
  "grounded": false,
  "evidence_pack": {
    "chunks": []
  },
  "retrieval_debug": {
    "k": 8,
    "chunks_retrieved": 3,
    "chunks_above_threshold": 0
  },
  "message": "No retrieved content supports a direct answer to this query."
}
```

**Grounding threshold:** 0.35 (min rank score)

### POST /v1/chunks

**Fetch specific chunks by ID.**

Request:
```json
{
  "context": "Chinvex",
  "chunk_ids": ["abc123", "def456"]
}
```

Response:
```json
{
  "context": "Chinvex",
  "chunks": [
    {
      "chunk_id": "abc123",
      "text": "...",
      "source_uri": "...",
      "source_type": "repo",
      "metadata": {...}
    }
  ]
}
```

**Limit:** Max 20 chunk IDs per request

### GET /v1/contexts

**List available contexts. Respects allowlist.**

Response:
```json
{
  "contexts": [
    {
      "name": "Chinvex",
      "aliases": ["chindex"],
      "updated_at": "2026-01-27T04:00:00Z"
    }
  ]
}
```

### POST /v1/answer (Optional, Flag-Gated)

**Full synthesis with LLM. Disabled by default.**

Enable with `GATEWAY_ENABLE_SERVER_LLM=true` or `gateway.json`:
```json
{
  "gateway": {
    "enable_server_llm": true
  }
}
```

Request:
```json
{
  "context": "Chinvex",
  "query": "How does score blending work?",
  "k": 8,
  "grounded": true
}
```

Response:
```json
{
  "schema_version": 1,
  "context": "Chinvex",
  "query": "How does score blending work?",
  "grounded": true,
  "answer": "Score blending combines normalized FTS and vector scores...",
  "citations": [
    {
      "chunk_id": "abc123",
      "source_uri": "C:\\Code\\chinvex\\src\\scoring.py",
      "range": {"line_start": 42, "line_end": 67}
    }
  ],
  "evidence_pack": {...},
  "errors": []
}
```

When disabled:
```json
{
  "error": "answer_endpoint_disabled",
  "message": "Server-side synthesis is disabled. Use /v1/evidence instead."
}
```

---

## Configuration

### gateway.json

**Location:** `P:\ai_memory\gateway.json`

```json
{
  "gateway": {
    "enabled": true,
    "host": "127.0.0.1",
    "port": 7778,
    "token_env": "CHINVEX_API_TOKEN",
    "context_allowlist": ["Chinvex", "Personal"],
    "cors_origins": [
      "https://chat.openai.com",
      "https://chatgpt.com"
    ],
    "rate_limit": {
      "requests_per_minute": 60,
      "requests_per_hour": 500
    },
    "limits": {
      "max_k": 20,
      "max_chunk_ids": 20,
      "max_query_length": 1000,
      "max_chunk_text_length": 5000
    },
    "enable_server_llm": false,
    "audit_log_path": "P:\\ai_memory\\gateway_audit.jsonl"
  }
}
```

### Environment Variables

| Variable | Purpose |
|----------|---------|
| `CHINVEX_API_TOKEN` | Bearer token for auth (required) |
| `GATEWAY_ENABLE_SERVER_LLM` | Enable `/v1/answer` endpoint |
| `CHINVEX_GATEWAY_CONFIG` | Path to gateway.json (optional) |

---

## CLI Commands

### Gateway Server

```bash
# Start gateway
chinvex gateway serve [--host HOST] [--port PORT] [--reload]

# Check status
chinvex gateway status
```

### Token Management

```bash
# Generate new token
chinvex gateway token generate
# Output: export CHINVEX_API_TOKEN=...

# Rotate token (show old, generate new)
chinvex gateway token rotate
```

---

## Deployment

### Option A: Cloudflare Tunnel (Recommended)

**Setup:**

```bash
# Install
winget install cloudflare.cloudflared

# Login and create tunnel
cloudflared tunnel login
cloudflared tunnel create chinvex

# Configure (~/.cloudflared/config.yml)
tunnel: chinvex
credentials-file: ~/.cloudflared/<tunnel-id>.json

ingress:
  - hostname: chinvex.yourdomain.com
    service: http://localhost:7778
  - service: http_status:404

# Add DNS
cloudflared tunnel route dns chinvex chinvex.yourdomain.com

# Run with PM2
pm2 start "cloudflared tunnel run chinvex" --name chinvex-tunnel
pm2 start "chinvex gateway serve --port 7778" --name chinvex-gateway
pm2 save
```

### Option B: Caddy Reverse Proxy

**Caddyfile:**

```
chinvex.yourdomain.com {
    reverse_proxy localhost:7778

    header {
        X-Content-Type-Options nosniff
        X-Frame-Options DENY
        X-XSS-Protection "1; mode=block"
        Strict-Transport-Security "max-age=31536000"
    }

    log {
        output file /var/log/caddy/chinvex.log
        format json
    }
}
```

**Start:**

```bash
pm2 start "chinvex gateway serve --port 7778" --name chinvex-gateway
caddy reload
```

---

## ChatGPT Integration

### 1. Create Custom GPT

1. Go to chat.openai.com → My GPTs → Create
2. Configure tab

**Name:** Chinvex Memory

**Instructions:**
```
You have access to the user's personal knowledge base via the Chinvex API.

CRITICAL RULES:
1. When the user asks about their projects, decisions, code, or past work, ALWAYS call getEvidence first.
2. If grounded=false, say "I couldn't find information about that in your memory." Do NOT make up an answer.
3. If grounded=true, synthesize an answer using ONLY the returned chunks. Cite sources.
4. Never claim to know something that isn't in the evidence pack.

When citing, use format: [source_uri:line_start-line_end]
```

### 2. Add Action

1. Actions → Create new action
2. Import from URL: `https://chinvex.yourdomain.com/openapi.json`
3. Authentication → API Key
   - Auth Type: Bearer
   - API Key: Your `CHINVEX_API_TOKEN`

### 3. Test

"Search my Chinvex memory for hybrid retrieval"

Expected: GPT calls `/v1/evidence`, synthesizes answer with citations

---

## Implementation Order

### Phase 1: Core Gateway (Days 1-2)

1. Project structure (`src/chinvex/gateway/`)
2. Configuration system (`config.py`, `gateway.json`)
3. Authentication middleware (`auth.py`)
4. Health endpoint (`/health`)
5. Evidence endpoint (`/v1/evidence`)
6. Search endpoint (`/v1/search`)
7. Chunks endpoint (`/v1/chunks`)
8. Contexts endpoint (`/v1/contexts`)

### Phase 2: Security (Day 2)

9. Context allowlist validation
10. Request parameter caps
11. Response truncation
12. Input validation (regex patterns)
13. Audit logging (`audit.py`)

### Phase 3: Configuration & CLI (Day 3)

14. Rate limiting (`rate_limit.py`)
15. CORS middleware
16. CLI: `chinvex gateway serve`
17. CLI: `chinvex gateway token generate`
18. CLI: `chinvex gateway token rotate`
19. CLI: `chinvex gateway status`

### Phase 4: Deployment (Days 3-4)

20. Cloudflare Tunnel docs
21. Caddy docs
22. Health check script
23. Environment setup guide

### Phase 5: ChatGPT Integration (Day 4)

24. OpenAPI schema customization
25. CORS testing
26. ChatGPT Action setup guide
27. End-to-end test

### Phase 6: Optional Answer Endpoint (Day 5)

28. `/v1/answer` implementation
29. Flag handling (env + config)
30. Tests (disabled/enabled states)

### Phase 7: Polish & Testing (Days 5-6)

31. Acceptance test script (12+ test cases)
32. Error handling audit
33. Performance testing
34. Documentation review

---

## File Structure

```
src/chinvex/
├── gateway/
│   ├── __init__.py
│   ├── app.py              # FastAPI application
│   ├── config.py           # Configuration loading
│   ├── auth.py             # Authentication
│   ├── rate_limit.py       # Rate limiting
│   ├── audit.py            # Audit logging
│   └── endpoints/
│       ├── __init__.py
│       ├── health.py       # GET /health
│       ├── search.py       # POST /v1/search
│       ├── evidence.py     # POST /v1/evidence
│       ├── chunks.py       # POST /v1/chunks
│       ├── contexts.py     # GET /v1/contexts
│       └── answer.py       # POST /v1/answer
├── cli.py                  # Gateway CLI commands
└── ...

docs/
├── plans/
│   └── 2026-01-27-p2-design.md
├── deployment/
│   ├── cloudflare-tunnel.md
│   └── caddy.md
└── chatgpt-integration.md

scripts/
└── test_gateway_p2.py      # Acceptance tests

P:\ai_memory\
├── gateway.json            # Gateway configuration
└── gateway_audit.jsonl     # Audit log
```

---

## Key Design Decisions

1. **Separate gateway.json** - Global config, not per-context
2. **Security first** - Auth, validation, rate limiting from day one
3. **Both deployment options** - Cloudflare Tunnel + Caddy docs
4. **Answer endpoint built but disabled** - Per spec
5. **Thin layer** - Gateway doesn't duplicate search logic
6. **In-memory rate limiting** - Redis deferred to P3
7. **Privacy-preserving audit log** - Query hashes, not raw text
8. **Context allowlist returns 404** - Not 403 (prevents enumeration)

---

## Acceptance Tests

| ID | Test | Pass Criteria |
|----|------|---------------|
| 2.5.1 | Health endpoint | Returns 200 with status=ok |
| 2.5.2 | No auth rejected | Returns 401 |
| 2.5.3 | Valid token accepted | Returns 200 |
| 2.5.4 | Invalid token rejected | Returns 401 |
| 2.5.5 | Evidence grounded=true | Chunks returned for known query |
| 2.5.6 | Evidence grounded=false | Message for unknown query |
| 2.5.7 | Search returns results | Ranked results with scores |
| 2.5.8 | Search respects k | results.length <= k |
| 2.5.9 | Unknown context | Returns 404 |
| 2.5.10 | k capped | results.length <= 20 |
| 2.5.11 | Invalid context name | Returns 400 |
| 2.5.12 | Answer disabled | Returns 403 when flag off |
| 2.5.13 | Answer enabled | Returns 200 when flag on |

---

*End of design document.*
